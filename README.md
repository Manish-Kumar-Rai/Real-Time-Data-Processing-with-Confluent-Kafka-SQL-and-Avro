# ðŸ›°ï¸ Real-Time Data Processing with Confluent Kafka, SQL, and Avro

This project demonstrates a real-time data processing pipeline using:
- **Microsoft SQL Server** as the source database
- **Confluent Kafka** as the streaming platform
- **Avro** as the serialization format
- **Python** for producer/consumer implementation

---

## ðŸ“ Project Structure

```
Real-Time-Data-Processing-with-Confluent-Kafka-SQL-and-Avro/
â”‚
â”œâ”€â”€ venv/                       # Python virtual environment (excluded from Git)
â”œâ”€â”€ .env                        # Environment variables (DB credentials, Kafka configs)
â”œâ”€â”€ .gitignore                  # Git ignore rules (venv/, __pycache__/, .env, etc.)
â”œâ”€â”€ README.md                   # You're here!
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ producer/                   # Kafka producer logic
â”‚   â”œâ”€â”€ kafka_producer.py       # Sends records to Kafka topic in Avro format
â”‚   â”œâ”€â”€ mssql_connector.py      # Connects to MSSQL and fetches new records
â”‚   â””â”€â”€ avro_schema.avsc        # Avro schema for serialization
â”‚   â””â”€â”€ checkpoints             # Directory to save last checkpoint 
â”‚
â”œâ”€â”€ consumer/                   # Kafka consumer logic
â”‚   â”œâ”€â”€ kafka_consumer.py       # Consumer group (5 members)
â”‚   â”œâ”€â”€ data_transform.py       # Business logic on consumed data
â”‚   â””â”€â”€ json_writer.py          # Writes transformed output as NDJSON
â”‚   â””â”€â”€ kafka_output            # Directory to save consumer outputs
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ config.yaml             # YAML for Kafka, DB, and file paths
â”‚   â””â”€â”€ logger.py               # common logger file 
â”‚
â”œâ”€â”€ images/                     #Screenshot of working project
â”‚   â””â”€â”€ screenshot.png          
â”œâ”€â”€ scripts/                    # SQL scripts for setup
â”‚   â”œâ”€â”€ create_table.sql        # Create product table
â”‚   â””â”€â”€ sample_inserts.sql      # Insert test records into table
```

---

## âš™ï¸ Setup Instructions

### 1. ðŸ§± Environment Setup
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. ðŸ“¦ Install Confluent Tools
Install Confluent Kafka + Schema Registry locally via Docker or manual setup.

- You must have these running:
  - Kafka broker
  - Zookeeper
  - Schema Registry

Refer: [https://docs.confluent.io/platform/current/quickstart/index.html](https://docs.confluent.io/platform/current/quickstart/index.html)

### 3. ðŸ—ƒï¸ SQL Server Setup
- Create the `products` table:
```bash
scripts/create_table.sql
```
- Insert test data:
```bash
scripts/sample_inserts.sql
```

### 4. ðŸ” Add Environment Variables
Create a `.env` file with the following:
```env
# MSSQL Database Configuration
DB_SERVER=your_server_name
DB_NAME=your_database_name
DB_USER=sa
DB_PASSWORD=your_db_password_here  # Replace with your actual password

# Confluent Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=your_bootstrap_server_link
KAFKA_SASL_MECHANISMS=PLAIN
KAFKA_SECURITY_PROTOCOL=SASL_SSL
KAFKA_USERNAME=your_kafka_username_here
KAFKA_PASSWORD=your_kafka_password_here

# Schema Registry Configuration
KAFKA_SCHEMA_REGISTRY_URL=your_registry_url
KAFKA_SCHEMA_REGISTRY_USERNAME=your_registry_username_here
KAFKA_SCHEMA_REGISTRY_PASSWORD=your_registry_password_here
```

---

### ðŸš€ CLI Commands to Run the Kafka Pipeline

```bash
# 1ï¸âƒ£ Insert sample data into the SQL Server table
python -m sqlScripts.insert_sample

# 2ï¸âƒ£ Run the Kafka Producer to fetch from SQL and push to Kafka
python -m kafka_producer.kafka_producer

# 3ï¸âƒ£ Start the Kafka Consumer to read, transform, and store the data
python -m kafka_consumer.kafka_consumer

---

## ðŸ“˜ Key Concepts

- **Incremental Fetching:** Fetch only new SQL rows using `last_updated > ?`
- **Avro Serialization:** Schema-based efficient messaging format
- **Kafka Consumer Group:** 5 consumers load-balanced across topic partitions
- **Business Logic Layer:** Apply transformations before storage
- **NDJSON Output:** JSON format with one record per line

---

## ðŸ” Example Config (config.yaml)
```yaml
queries:
  incremental_fetch: >
    SELECT id,name,category,price,last_updated
    FROM products
    WHERE last_updated > ?
    ORDER BY last_updated ASC

  last_record: >
    SELECT TOP 1 id,name,category,price,last_updated
    FROM products
    ORDER BY id DESC
```

---

## âœ… Output Example (JSON Line)
```json
{"id": 1, "name": "Laptop", "category": "Electronics", "price": 576.89, "last_updated": "2025-08-03T14:21:33"}
```

---

## ðŸ§  Authors & Credits
Developed by **Manish Kumar Rai**.

---

## ðŸ“„ License
MIT License â€“ Free to use and modify.
